{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682f35f1",
   "metadata": {},
   "source": [
    "# CSC 8614 - Language Models\n",
    "## CI3 - Parameter-Efficient Fine-Tuning with LoRA\n",
    "\n",
    "This TP builds upon the GPT architecture we have previously explored. \n",
    "\n",
    "We will implement Low-Rank Adaptation (LoRA) from scratch and inject it into our pre-existing GPTModel.\n",
    "\n",
    "Objectives:\n",
    "- Implement the mathematical formulation of LoRA.\n",
    "- Create a wrapper to convert standard Linear layers into LoRA-compatible layers.\n",
    "- Inject these layers into a pre-trained GPT model.\n",
    "- Verify that only a fraction of parameters are trainable.\n",
    "- Fine-tune the model with LoRA\n",
    "\n",
    "Some of this code comes from the book _Build a Large Language Model (From Scratch)_, by Sebastian Raschka, and its [official github repository](https://github.com/rasbt/LLMs-from-scratch).\n",
    "\n",
    "This TP will be done in this notebook, and requires some additional files (available from the course website). \n",
    "You will have to fill the missing portions of code, and perform some additional experiments by testing different parameters.\n",
    "\n",
    "Working on this TP:\n",
    "- The easiest way is probably to work directly on the notebook, using jupyter notebook or visual studio code. An alternative is also to use Google colab.\n",
    "- You should be able to run everything on your machine, but you can connect to the GPUs if needed.\n",
    "- **NOTE**: run the cells in the correct order, otherwise you might get errors due to inconsintencies.\n",
    "\n",
    "Some files are required, and are available on the course website and/or github repo:\n",
    "- `requirements.txt`\n",
    "- `gpt_utils.py`\n",
    "\n",
    "\n",
    "## About the report\n",
    "You will have to return this notebook (completed), as well as a mini-report (`TP3/rapport.md`).\n",
    "\n",
    "The notebook and report shall be submitted via a GitHub repository, similarly to what you did for the previous sessions (remember to use a different folder: `TP3`).\n",
    "For the notebook, it is sufficient to complete the code and submit the final version.\n",
    "\n",
    "For the mini-report, you have to answer the questions asked in this notebook, and discuss some of your findings as requested.\n",
    "Same as in the previous sessions:\n",
    "- You must include: short answers, observed results (copies of outputs), requested screenshots, and a brief interpretation.\n",
    "- Do not paste entire pages: be concise and select the relevant elements.\n",
    "\n",
    "Reproducibility: \n",
    "- fix a random seed and write it in the report\n",
    "- indicate in the report the specific python version OS, and the library versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8ead2",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "Install the requirements.\n",
    "\n",
    "**Note**: if you use the same virtual environment as last time, you will not have to reinstall everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347af8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979fc7",
   "metadata": {},
   "source": [
    "## Background & Mathematical Formulation\n",
    "\n",
    "Fine-tuning Large Language Models (LLMs) updates all model parameters, which is computationally expensive. LoRA freezes the pre-trained weights and injects trainable rank decomposition matrices.\n",
    "\n",
    "Given a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d_{out} \\times d_{in}}$, LoRA constrains the update $\\Delta W$ by representing it with a low-rank decomposition:\n",
    "\n",
    "$$W_0 + \\Delta W = W_0 + B A$$\n",
    "\n",
    "Where:\n",
    "- $B \\in \\mathbb{R}^{d_{out} \\times r}$\n",
    "- $A \\in \\mathbb{R}^{r \\times d_{in}}$\n",
    "- $r \\ll \\min(d_{in}, d_{out})$ is the rank.\n",
    "\n",
    "The Forward Pass:\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} (B A x)$$\n",
    "\n",
    "- $\\alpha$ is a scaling constant.\n",
    "- $A$ is initialized with random Gaussian values.\n",
    "- $B$ is initialized with zeros (so training starts with no changes to the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbec02b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and our provided GPT utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15a71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from gpt_utils import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685e3ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b68ccf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a222d",
   "metadata": {},
   "source": [
    "## Implementing the LoRA Layer\n",
    "\n",
    "In this exercise, you will create the LoRALayer module. This module computes the $\\Delta Wx$ term (the branch on the right side of the diagram seen during the lecture).\n",
    "\n",
    "### **Exercise 1**: Define the LoRA Module\n",
    "\n",
    "Requirements:\n",
    "1. Define dimensions for parameters A and B based on in_dim, out_dim, and rank.\n",
    "2. Initialize A with kaiming_uniform_ (or small random normal).\n",
    "3. Initialize B with zeros.\n",
    "4. Implement the forward pass including the scaling factor $\\frac{\\alpha}{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be539ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank  # Scaling factor per LoRA paper\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Initialize A and B\n",
    "        # A maps from in_dim -> rank\n",
    "        # B maps from rank -> out_dim\n",
    "        # Hint: use nn.Parameter\n",
    "        # ---------------------------------------------------------\n",
    "        self.A = nn.Parameter(torch.empty(rank, in_dim))      # (rank, in_dim)\n",
    "        self.B = nn.Parameter(torch.empty(out_dim, rank))     # (out_dim, rank)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Apply Initializations\n",
    "        # A: Kaiming Uniform (a=sqrt(5))\n",
    "        # B: Zeros\n",
    "        # ---------------------------------------------------------\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Calculate the LoRA output\n",
    "        # ---------------------------------------------------------\n",
    "        result = (x @ self.A.t() @ self.B.t()) * self.scaling\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767c887",
   "metadata": {},
   "source": [
    "## Wrapping Linear Layers\n",
    "\n",
    "We rarely replace the layer entirely; instead, we want a layer that holds both the frozen original weights and the new LoRA weights.\n",
    "\n",
    "### **Exercise 2**: The LinearWithLoRA Wrapper\n",
    "\n",
    "Requirements:\n",
    "1. Store the original linear layer.\n",
    "2. Create a self.lora instance using the dimensions of the original linear layer.\n",
    "3. In forward, add the output of the original layer to the output of the LoRA layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d268ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed: Wrapper acts identically to original layer at initialization.\n"
     ]
    }
   ],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Instantiate the LoRALayer\n",
    "        # Use linear_layer.in_features and linear_layer.out_features\n",
    "        # ---------------------------------------------------------\n",
    "        self.lora = LoRALayer(\n",
    "            in_dim=linear_layer.in_features,\n",
    "            out_dim=linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Combine Frozen + LoRA paths\n",
    "        # ---------------------------------------------------------\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# --- Sanity Check ---\n",
    "# Create a dummy layer to test\n",
    "input_x = torch.randn(1, 128)\n",
    "original_layer = nn.Linear(128, 64)\n",
    "lora_wrapped = LinearWithLoRA(original_layer, rank=4, alpha=8)\n",
    "\n",
    "# Because B is initialized to zeros, the outputs should be identical initially\n",
    "assert torch.allclose(original_layer(input_x), lora_wrapped(input_x))\n",
    "print(\"Test Passed: Wrapper acts identically to original layer at initialization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201146a1",
   "metadata": {},
   "source": [
    "## Injecting LoRA into GPTModel\n",
    "\n",
    "Now we need to modify our existing GPTModel. We cannot manually rewrite the class. Instead, we will iterate through the model's modules and replace specific layers dynamically.\n",
    "\n",
    "In GPTModel, the transformer blocks are stored in *self.trf_blocks*. Inside those, we have attention mechanisms (att) containing W_query, W_key, W_value, or a combined c_attn.\n",
    "\n",
    "Note: For this lab, to keep it simple, we will replace all nn.Linear layers except the final output head.\n",
    "\n",
    "### **Exercise 3**: Recursive Model Modification\n",
    "\n",
    "Requirements:\n",
    "1. Iterate through named children of the model.\n",
    "2. If a module is nn.Linear, wrap it in LinearWithLoRA.\n",
    "3. Important: Skip the final output layer (often named out_head or similar in gpt_utils), as we usually don't want to reduce the rank of the vocabulary projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116d7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    \"\"\"\n",
    "    Recursively replaces nn.Linear with LinearWithLoRA.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # ---------------------------------------------------------\n",
    "            # TODO: Add logic to skip the final output head\n",
    "            # ---------------------------------------------------------\n",
    "            if name == \"out_head\":\n",
    "                continue\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # TODO: Replace the layer\n",
    "            # 1. Wrap the current module (using LinearWithLoRA)\n",
    "            # 2. Assign it back to the model using setattr()\n",
    "            # ---------------------------------------------------------\n",
    "            new_layer = LinearWithLoRA(module, rank=rank, alpha=alpha)\n",
    "            setattr(model, name, new_layer)\n",
    "\n",
    "        else:\n",
    "            # Recursive call for nested modules (like TransformerBlocks)\n",
    "            replace_linear_with_lora(module, rank, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5f1d3",
   "metadata": {},
   "source": [
    "## Freeze & Verify\n",
    "\n",
    "We have injected the layers, but currently, everything is still trainable. We must freeze the original weights.\n",
    "\n",
    "### **Exercise 4**: Freezing and Counting Parameters\n",
    "Requirements:\n",
    "1. Set requires_grad = False for all parameters.\n",
    "2. Iterate through the model; if a layer is LinearWithLoRA, unfreeze self.lora.A and self.lora.B.\n",
    "3. Calculate the ratio of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5764f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_and_activate_lora(model):\n",
    "    # ---------------------------------------------------------\n",
    "    # TODO: Freeze ALL parameters in the model\n",
    "    # ---------------------------------------------------------\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # TODO: Unfreeze only LoRA A and B matrices\n",
    "    # Hint: iterate through model.modules() and check LinearWithLoRA\n",
    "    # ---------------------------------------------------------\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.lora.A.requires_grad = True\n",
    "            module.lora.B.requires_grad = True\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    \n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            \n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab6916",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's download and initialize the model from gpt_utils, then apply our LoRA transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73aae7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 83.6kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 918kiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 78.6kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:43<00:00, 11.5MiB/s]  \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 2.46MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 767kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 699kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights downloaded and loaded into memory.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "# (Basically same code as last session)\n",
    "\n",
    "from gpt_utils import GPTModel, download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "# Download the model weights (124M param version), and initialize it.\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2_weights\")\n",
    "print(\"Weights downloaded and loaded into memory.\")\n",
    "\n",
    "# Configure the model, mapping OpenAI specific keys to our model's keys (if needed)\n",
    "model_config = {\n",
    "    \"vocab_size\": settings[\"n_vocab\"],\n",
    "    \"context_length\": settings[\"n_ctx\"],\n",
    "    \"emb_dim\": settings[\"n_embd\"],\n",
    "    \"n_heads\": settings[\"n_head\"],\n",
    "    \"n_layers\": settings[\"n_layer\"],\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True,\n",
    "}\n",
    "\n",
    "# Initialize the Base Model\n",
    "model = GPTModel(model_config)\n",
    "load_weights_into_gpt(model, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26ffba",
   "metadata": {},
   "source": [
    "Now, we call all the methods we have defined above, and put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4001e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Structure (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Model Structure After LoRA (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_key): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_value): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (out_proj): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (1): GELU()\n",
      "      (2): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Parameter Count:\n",
      "trainable params: 1,327,104 || all params: 164,364,288 || trainable%: 0.81%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "print(\"Original Model Structure (Truncated):\")\n",
    "print(model.trf_blocks[0]) # Print first block to see standard Linear layers\n",
    "\n",
    "# Apply LoRA Replacement\n",
    "# Rank 8, Alpha 16 (Alpha is usually set to 2x Rank as a rule of thumb)\n",
    "replace_linear_with_lora(model, rank=8, alpha=16)\n",
    "\n",
    "# 3. Freeze Weights\n",
    "freeze_and_activate_lora(model)\n",
    "\n",
    "# 4. Check Results\n",
    "print(\"\\nModel Structure After LoRA (Truncated):\")\n",
    "print(model.trf_blocks[0])\n",
    "\n",
    "print(\"\\nParameter Count:\")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a264553",
   "metadata": {},
   "source": [
    "**Question 1:** Do you see any difference between \"Original Model Structure (Truncated)\" and \"Model Structure After LoRA (Truncated)\"? Do you see the LinearWithLoRA you have defined above?\n",
    "\n",
    "**Question 2:** What is the number of trainable parameters, all parameters, and the fraction of trainable parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e97c6",
   "metadata": {},
   "source": [
    "## Training Loop Verification\n",
    "\n",
    "Finally, let's prove that gradients are only generated for the specific LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8076639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Verification:\n",
      "Parameters with gradients: 144\n",
      "Frozen parameters correctly without gradients: 197\n",
      "SUCCESS: Gradients are flowing correctly only into LoRA parameters.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 2\n",
    "dummy_input = torch.randint(0, 1000, (batch_size, 256))\n",
    "dummy_target = torch.randint(0, 1000, (batch_size, 256))\n",
    "\n",
    "# Optimizer setup\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-3)\n",
    "\n",
    "# Forward Pass\n",
    "logits = model(dummy_input)\n",
    "loss = torch.nn.functional.cross_entropy(logits.view(-1, model_config['vocab_size']), dummy_target.view(-1))\n",
    "\n",
    "# Backward Pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Verification Step\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nGradient Verification:\")\n",
    "grads_found = 0\n",
    "grads_missing = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param.grad is not None:\n",
    "            grads_found += 1\n",
    "        else:\n",
    "            print(f\"WARNING: Trainable parameter {name} has no gradient.\")\n",
    "    else:\n",
    "        if param.grad is not None:\n",
    "            print(f\"ERROR: Frozen parameter {name} has a gradient!\")\n",
    "        else:\n",
    "            grads_missing += 1\n",
    "\n",
    "print(f\"Parameters with gradients: {grads_found}\")\n",
    "print(f\"Frozen parameters correctly without gradients: {grads_missing}\")\n",
    "\n",
    "if grads_found > 0 and grads_missing > 0:\n",
    "    print(\"SUCCESS: Gradients are flowing correctly only into LoRA parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff671a8",
   "metadata": {},
   "source": [
    "## SPAM Classification\n",
    "\n",
    "Let's now work the Spam Classification task again, but this time using the LoRA-adapted model. \n",
    "This follows the example from the previous session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933e138",
   "metadata": {},
   "source": [
    "### Download and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8beff640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset downloaded and extracted.\n",
      "Loaded 5572 examples.\n",
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Balanced Dataset size: 1494\n",
      "Size of train_df: 1195\n",
      "Size of test_df: 299\n",
      "Distribution of labels in the dataset:\n",
      "train:\n",
      "label\n",
      "1    612\n",
      "0    583\n",
      "Name: count, dtype: int64\n",
      "test:\n",
      "label\n",
      "0    164\n",
      "1    135\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = os.path.join(extracted_path, \"SMSSpamCollection\")\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    print(\"Dataset downloaded and extracted.\")\n",
    "\n",
    "# Load into DataFrame\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "print(f\"Loaded {len(df)} examples.\")\n",
    "print(df.head())\n",
    "\n",
    "# Class Balancing (primarily for speed in the lab)\n",
    "spam_df = df[df[\"label\"] == \"spam\"]\n",
    "ham_df = df[df[\"label\"] == \"ham\"].sample(len(spam_df), random_state=RANDOM_STATE)\n",
    "df = pd.concat([spam_df, ham_df]).reset_index(drop=True)\n",
    "\n",
    "# Map labels to integers\n",
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "print(f\"Balanced Dataset size: {len(df)}\")\n",
    "\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "print(f\"Size of train_df: {len(train_df)}\")\n",
    "print(f\"Size of test_df: {len(test_df)}\")\n",
    "\n",
    "print(\"Distribution of labels in the dataset:\")\n",
    "print(\"train:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"test:\")\n",
    "print(test_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb81f48",
   "metadata": {},
   "source": [
    "Define Dataset and DataLoader, similarly to previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b33971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "        \n",
    "        # Truncate\n",
    "        encoded = encoded[:self.max_length]\n",
    "        \n",
    "        # Pad (GPT-2 usually uses <|endoftext|> as padding)\n",
    "        pad_len = self.max_length - len(encoded)\n",
    "        encoded = encoded + [50256] * pad_len # 50256 is <|endoftext|> in GPT2\n",
    "        \n",
    "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Create Loaders\n",
    "train_loader = DataLoader(SpamDataset(train_df, tokenizer), batch_size=8, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(SpamDataset(test_df, tokenizer), batch_size=8, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b9dc8",
   "metadata": {},
   "source": [
    "We now modify the Model for Classification, replacing the final layer (out_head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e686056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Output Head: Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# Check input dimension of the current head\n",
    "hidden_dim = model.out_head.in_features\n",
    "\n",
    "# Replace the head\n",
    "model.out_head = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "# Move model to device (if using GPU, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"New Output Head:\", model.out_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4b703",
   "metadata": {},
   "source": [
    "We previously froze everything except LoRA. Now we added a new head, let's make it unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6889edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,328,642 || all params: 125,768,450 || trainable%: 1.06%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def set_classification_trainable(model):\n",
    "    # Ensure LoRA layers are trainable (A and B)\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.lora.A.requires_grad = True\n",
    "            module.lora.B.requires_grad = True\n",
    "    \n",
    "    # Ensure the new classification head is trainable\n",
    "    for param in model.out_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "set_classification_trainable(model)\n",
    "\n",
    "# Verify count again\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dec47a",
   "metadata": {},
   "source": [
    "**Question 3:** Check the number (and fraction) of trainable parameters, and compare it with the one above. Do you see any differences? Can you describe them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5f0ac",
   "metadata": {},
   "source": [
    "The Training Loop\n",
    "Context: Standard PyTorch loop. Note that GPT models output [batch, seq_len, hidden]. For classification, we usually take the hidden state of the last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c03d3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import time\n",
    "\n",
    "def train_classifier(model, loader, optimizer, device, epochs=1):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            # The model outputs (batch, seq_len, num_classes)\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Select the last token for classification\n",
    "            # last_token_logits = logits[:, -1, :]  \n",
    "            # NOTE: this (the line above) was an error in the code provided with the previous lab: it was using the last token (which is most often PAD), \n",
    "            #   not the last non padding token. \n",
    "            # Select the last NON-PADDING token\n",
    "            #   Create a mask (1 for real tokens, 0 for PAD); 50256 is the PAD token ID\n",
    "            mask = (inputs != 50256)\n",
    "            \n",
    "            # Find the index of the last real token\n",
    "            #    Summing the mask gives the length. Subtract 1 for 0-based index.\n",
    "            #    .clamp(min=0) prevents errors if a sequence is empty (unlikely)\n",
    "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
    "            \n",
    "            # Select the logits at those specific indices\n",
    "            #    We use torch.arange for the batch dimension\n",
    "            batch_indices = torch.arange(inputs.size(0), device=device)\n",
    "            last_token_logits = logits[batch_indices, last_idx, :]\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(last_token_logits, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy for monitoring\n",
    "            preds = torch.argmax(last_token_logits, dim=-1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        acc = correct / total * 100\n",
    "        print(f\"Epoch {epoch+1} Finished | Avg Loss: {total_loss/len(loader):.4f} | Acc: {acc:.2f}% | Time: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d347648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 0 | Loss: 2.9491\n",
      "Epoch 1 | Batch 10 | Loss: 1.3096\n",
      "Epoch 1 | Batch 20 | Loss: 0.2150\n",
      "Epoch 1 | Batch 30 | Loss: 0.3128\n",
      "Epoch 1 | Batch 40 | Loss: 0.0593\n",
      "Epoch 1 | Batch 50 | Loss: 0.0281\n",
      "Epoch 1 | Batch 60 | Loss: 0.0237\n",
      "Epoch 1 | Batch 70 | Loss: 0.0116\n",
      "Epoch 1 | Batch 80 | Loss: 0.0032\n",
      "Epoch 1 | Batch 90 | Loss: 0.0016\n",
      "Epoch 1 | Batch 100 | Loss: 0.0043\n",
      "Epoch 1 | Batch 110 | Loss: 0.0035\n",
      "Epoch 1 | Batch 120 | Loss: 0.2439\n",
      "Epoch 1 | Batch 130 | Loss: 0.3970\n",
      "Epoch 1 | Batch 140 | Loss: 0.0324\n",
      "Epoch 1 Finished | Avg Loss: 0.2789 | Acc: 92.79% | Time: 301.66s\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Setup Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad], \n",
    "    lr=5e-4    # TODO: Potentially test with different learning rates\n",
    ")\n",
    "\n",
    "# Run Training\n",
    "train_classifier(model, train_loader, optimizer, device, epochs=1)  # TODO: Potentially test with different numbers of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02917255",
   "metadata": {},
   "source": [
    "**Question 4:** Can you describe the trend of the loss, and the final accuracy. Is it reasonable considering the task at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e655eb",
   "metadata": {},
   "source": [
    "We can now test the accuracy on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd866e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def evaluate_accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Select last \"real\" token logits\n",
    "            mask = (inputs != 50256)\n",
    "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
    "            batch_idx = torch.arange(inputs.size(0), device=device)\n",
    "            last_token_logits = logits[batch_idx, last_idx, :]\n",
    "            # Select last token logits (same logic as training)\n",
    "            # last_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            # Compare with targets\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553c3a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 97.66%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Run evaluation\n",
    "test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
    "print(f\"Test Set Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713ca7",
   "metadata": {},
   "source": [
    "**Question 5:** How is the accuracy, and how does it compare to the Train set accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1b259",
   "metadata": {},
   "source": [
    "Finally, we can do a quick inference test, to see how the model classifies new texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f06b8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def classify_text(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded = encoded[:256] # Truncate\n",
    "    tensor_input = torch.tensor([encoded], dtype=torch.long).to(device) # Add batch dim\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_input)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).item()\n",
    "        \n",
    "    label_map = {0: \"HAM (Normal)\", 1: \"SPAM\"}\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Prediction: {label_map[pred_label]} (Confidence: {probs[0][pred_label]:.2f})\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference quick test (add any texts you want)\n",
    "classify_text(\"There is a big cash prize for you, call immediately.\", model, tokenizer, device)\n",
    "classify_text(\"Hey, are we still meeting for lunch tomorrow?\", model, tokenizer, device)\n",
    "\n",
    "# Additional examples\n",
    "classify_text(\"URGENT! You have won a free iPhone. Click this link to claim now.\", model, tokenizer, device)\n",
    "classify_text(\"Can you send me the notes from today's class when you have time?\", model, tokenizer, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4d416",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
